#  大数据

## HDFS的架构详解

![1271254-20190925195925515-846526737](D:\技术学习\learning\笔记\学习\我的笔记图片\1271254-20190925195925515-846526737.png)

- **Namenode**：负责管理文件目录、文件和block的对应关系以及block和datanode的对应关系，维护目录树，接管用户的请求，如下图：

  ![1271254-20190925200117604-1562843813](D:\技术学习\learning\笔记\学习\我的笔记图片\1271254-20190925200117604-1562843813.png)

- **DataNode**：（数据节点）管理连接到它们运行的节点的存储，负责处理来自文件系统客户端的读写请求。DataNodes还执行块创建，删除

- **Client**：(客户端)代表用户通过与nameNode和datanode交互来访问整个文件系统，HDFS对外开放文件命名空间并允许用户数据以文件形式存储。用户通过客户端（Client）与HDFS进行通讯交互。

#### **块和复制**

块和复本在hdfs架构中分布如下图所示

![1271254-20190925203009451-770869928](D:\技术学习\learning\笔记\学习\我的笔记图片\1271254-20190925203009451-770869928.png)

#### **容错机制**

- **SencondaryNameNode**:  职责是合并NameNode的edit logs到fsimage文件中。namenode和SencondaryNameNode都注册在zookeeper进行监听。

  1.首先，它定时到NameNode去获取edit logs，并更新到fsimage上。[笔者注：Secondary NameNode自己的fsimage]

  2.一旦它有了新的fsimage文件，它将其拷贝回NameNode中。

  3.NameNode在下次重启时会使用这个新的fsimage文件，从而减少重启的时间。

![1271254-20190925200933795-1566638215](D:\技术学习\learning\笔记\学习\我的笔记图片\1271254-20190925200933795-1566638215.png)

#### **HDFS读取数据流程**

HDFS读数据流程如下图所示：

![1271254-20190925201454324-822546854-1](D:\技术学习\learning\笔记\学习\我的笔记图片\1271254-20190925201454324-822546854-1.png)

1、客户端通过FileSystem对象（DistributedFileSystem）的open()方法来打开希望读取的文件。

2、DistributedFileSystem通过远程调用（RPC）来调用namenode，获取到每个文件的起止位置。对于每一个块，namenode返回该块副本的datanode。这些datanode会根据它们与客户端的距离（集群的网络拓扑结构）排序，如果客户端本身就是其中的一个datanode，那么就会在该datanode上读取数据。DistributedFileSystem远程调用后返回一个FSDataInputStream（支持文件定位的输入流）对象给客户端以便于读取数据，然后FSDataInputStream封装一个DFSInputStream对象。该对象管理datanode和namenode的IO。

3、客户端对这个输入流调用read()方法，存储着文件起始几个块的datanode地址的DFSInputStream随即连接距离最近的文件中第一个块所在的datanode，通过数据流反复调用read()方法，可以将数据从datanode传送到客户端。当读完这个块时，DFSInputStream关闭与该datanode的连接，然后寻址下一个位置最佳的datanode。

#### HDFS的写数据流程

HDFS写数据流程图如下图所示：

![1271254-20190925201846873-136405785](D:\技术学习\learning\笔记\学习\我的笔记图片\1271254-20190925201846873-136405785.png)

1、首先客户端通过DistributedFileSystem上的create()方法指明一个预创建的文件的文件名

2、DistributedFileSystem再通过RPC调用向NameNode申请创建一个新文件（这时该文件还没有分配相应的block）。namenode检查是否有同名文件存在以及用户是否有相应的创建权限，如果检查通过，namenode会为该文件创建一个新的记录，否则的话文件创建失败，客户端得到一个IOException异常。DistributedFileSystem返回一个FSDataOutputStream以供客户端写入数据，与FSDataInputStream类似，FSDataOutputStream封装了一个DFSOutputStream用于处理namenode与datanode之间的通信。

3、当客户端开始写数据时（，DFSOutputStream把写入的数据分成包（packet）, 放入一个中间队列——数据队列（data  queue）中去。DataStreamer从数据队列中取数据，同时向namenode申请一个新的block来存放它已经取得的数据。namenode选择一系列合适的datanode（个数由文件的replica数决定）构成一个管道线（pipeline），这里我们假设replica为3，所以管道线中就有三个datanode。

4、DataSteamer把数据流式的写入到管道线中的第一个datanode中，第一个datanode再把接收到的数据转到第二个datanode中，以此类推。

5、DFSOutputStream同时也维护着另一个中间队列——确认队列（ack queue），确认队列中的包只有在得到管道线中所有的datanode的确认以后才会被移出确认队列

如果某个datanode在写数据的时候当掉了，下面这些对用户透明的步骤会被执行：

​    管道线关闭，所有确认队列上的数据会被挪到数据队列的首部重新发送，这样可以确保管道线中当掉的datanode下流的datanode不会因为当掉的datanode而丢失数据包。

​    在还在正常运行的datanode上的当前block上做一个标志，这样当当掉的datanode重新启动以后namenode就会知道该datanode上哪个block是刚才当机时残留下的局部损坏block，从而可以把它删掉。

​     已经当掉的datanode从管道线中被移除，未写完的block的其他数据继续被写入到其他两个还在正常运行的datanode中去，namenode知道这个block还处在under-replicated状态（也即备份数不足的状态）下，然后他会安排一个新的replica从而达到要求的备份数，后续的block写入方法同前面正常时候一样。有可能管道线中的多个datanode当掉（虽然不太经常发生），但只要dfs.replication.min（默认为1）个replica被创建，我们就认为该创建成功了。剩余的replica会在以后异步创建以达到指定的replica数。

6、当客户端完成写数据后，它会调用close()方法。这个操作会冲洗（flush）所有剩下的package到pipeline中。

7、等待这些package确认成功，然后通知namenode写入文件成功。这时候namenode就知道该文件由哪些block组成（因为DataStreamer向namenode请求分配新block，namenode当然会知道它分配过哪些blcok给给定文件），它会等待最少的replica数被创建，然后成功返回。

- 注意：hdfs写入过程中，datanode管线的确认应答包并不是每写完一个datanode，就返回一个确认应答，而是一直写入，直到最后一个datanode写入完毕后，统一返回应答包。如果中间的一个datanode出现故障，那么返回的应答就是前面完好的datanode确认应答，和故障datanode的故障异常。

#### 为什么会有小文件

​	小文件是指文件size小于HDFS上block大小的文件，shuffle的reducetask设置不合理（应为一个reduce产生一个文件，设置过多（partition和reduce数量对应）回产生过多小文件）



#### 小文件带来的问题

- 在HDFS中，任何block，文件或者目录在内存中均以对象（元数据）的形式存储，每个对象约占200byte，如果有1000 0000个小文件，每个文件占用一个block，则namenode大约需要2G空间。如果存储1亿个文件，则namenode需要20G空间，大量小文件的存在势必占用大量的 NameNode 内存，从而影响 HDFS 的横向扩展能力。

- 如果我们使用 MapReduce 任务来处理这些小文件，因为每个 Map 会处理一个 HDFS 块；这会导致程序启动大量的 Map 来处理这些小文件，虽然这些小文件总的大小并非很大，却占用了集群的大量资源！

#### 小文件解决方案

- 小文件处理模块的设计思想是，先将很多小文件合并成一个大文件，然后为这些小文件建立索引，以便进行快速存取和访问。

- 论文结合WebGIS中数据相关性特征，将保存相邻地理位置信息的小文件合并成一个大的文件，并为这些小文件建立索引以便对小文件进行存取。

总结：它自带的三种方案，包括Hadoop Archive，Sequence file和CombineFileInputFormat，需要用户根据自己的需要编写程序解决小文件问题；而第四节提到的论文均是针对特殊应用提出的解决方案，没有形成一个比较通用的技术方案。

#### hadoop配置文件详解

- core-site.xml配置namenode和临时目录，./hdfs namenode -format

![1590504605(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590504605(1).jpg)

- hdfs-site.xml配置副本系数

![1590504724(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590504724(1).jpg)

- slaves配置从节点

- mapred-site.xml配置使用yarn做资源调度

  ![1590504869(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590504869(1).jpg)

- yarn-site.xml

  ![1590504980(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590504980(1).jpg)



yarn的测试用例在$HADOOP_HOME/share/hadoop/mapreduce,里面有很多jar包

## SQL on Hadoop

#### 行式存储和列式存储

- **行式存储**

  优点：

  ​	同一行数据存放在同一个block块里面，select * from table_name;数据能直接获取出，INSERT/UPDATE比	较方便

  缺点：

  ​	不同类型数据存放在同一个block块里面，压缩性能不好；

  ​	select id,name from table_name;这种类型的列查询，所有数据都要读取，而不能跳过。

- 列式存储

  优点：

  ​	同类型数据存放在同一个block块里面，压缩性能好；

  ​	任何列都能作为索引。

  缺点：

  ​	select * from table_name;这类全表查询，需要数据重组；

  ​	INSERT/UPDATE比较麻烦。

#### 架构调优

- 分表 ：对于行式存储，经常读取到不需要的列，分表把不同的列分到不同的表中，比如把访问频繁的列放到一个表中，这样就可以切分大表，提高性能。
- 分区表：按照日期来对表进行分区，这样查询得时候就会减少数据量，比如以天来进行分区，分区可以分单级分区和多级分区（字面意思），静态分区和动态分区。
- 充分利用中间结果集：把经常访问到的列放到一张表中，IO负载降低，提升作业执行性能，缺点：占用更多的磁盘内存空间。

**压缩调优**

常用的压缩算法：使用场景：输入数据，中间数据，输出数据

![1590035654(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590035654(1).jpg)

![1590036068(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590036068(1).jpg)

在core-site.xml中的配置

![1590037124(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590037124(1).jpg)

可以在mapred-site.xml配置压缩算法

![1590036275(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590036275(1).jpg)

结合hive压缩

set hive.exec.compress.output=ture

set mapreduce.output.fileoutputformat.conmpress.codes=org.apache.hadoop.io.compress.BZop2Codec;

## Spark



#### 语法调优

- order by和sort by区别：order by保证全局有序，sort by 保证分区内有序（每个reduce有序，不同reducetask之间无序）

- distribute by 和cluster by的区别：

  DISTRIBUTE BY 控制map 中的输出在 reducer 中是如何进行划分的。使用DISTRIBUTE BY 可以保证相同KEY的记录被划分到一个Reduce 中。

  distribute by 和 sort by 合用就相当于cluster by，但是cluster by 不能指定排序为asc或 desc 的规则，只能是升序排列。

- 调整reduce的个数：

  reduce数量很多，理论上小文件出现概率很大，

  reduce数量很少，运行会很慢，耗时，数据倾斜

  设置参数：hive.exec.reducer,bytes.per.reducer(每个reduce能用到的内存，0.14.0以前市一个reduce1G，0.14.0以后市一个reduce256M）

  hive.exec.reducers.max（指定reduce的最大数量）

  mapred.reduce.tasks = 3(指定reduce的数量，hive reducer= spark sql partitions(200))

- 执行计划： explain加sql语句，由三部分组成

  1，抽象语法树(加extended)

  2，每个stages之间的依赖关系

  3，每个stage的详细描述

- mapreduce join

  普通的join（reduce join）：在map阶段, 把关键字作为key输出，并在value中标记出数据是来自data1还是data2。因为在shuffle阶段已经自然按key分组，reduce阶段，判断每一个value是来自data1还是data2,在内部分成2组，做集合的乘积。

  缺点：

  ​	1, map阶段没有对数据瘦身，shuffle的网络传输和排序性能很低。
  ​	2, reduce端对2个集合做乘积计算，很耗内存，容易导致OOM。

  mapjoin （hive 默认的join， hive.aotu.convert.join = true) : 

  select /* + MAPJOIN（d）*/  colomn from table;标识哪个表作为小表放到Distributedcache中

  1，把小表加载到缓存中

  2，通过mapper区读取大表的数据

  3，在读取大表数据的记录时和缓存中的小标数据直接进行对比

  4，join上就ok

  这种方法，要使用Hadoop中的DistributedCache（放在Hashtable Files中）把小数据分布到各个计算节点，每个map节点都要把小数据库加载到内存，按关键字建立索引。

  局限性：

  ​	有一份数据比较小，在map端，能够把它加载到内存，并进行join操作。

- 推测执行

  一个job由100reducer，99个运行的 很快，但是有一个运行得很慢，木桶原理，速度取决于最慢得task，推测运行就是Hadoop会为该task启动备份任务，让speculative task与原始task同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果，并且在运行完成后Kill掉另外一个任务。

  缺点：

  在资源很紧张的情况下，推测执行不一定能带来时间上的优化。如果在资源本身就不够的情况下，还要跑推测执行的任务，这样会导致后续启动的任务无法获取到资源，以导致无法执行

- 并行执行

  set hive.exec.parallel = ture(默认为false)

  不同的jobs并行执行

  set hive.exec.parallel.thread.number(设置并行数目 default value：8)

- JVM重用

  mapred.job.reuse.jvm.num.tasks（甚至重用数目）

  这表示属于同一job的顺序执行的task可以共享一个JVM，也就是说第二轮的map可以重用前一轮的JVM，而不是第一轮结束后关闭JVM，第二轮再启动新的JVM。如果设置成-1，那么只要是同一个job的task（无所谓多少个），都可以按顺序在一个JVM上连续执行。

  JVM重用技术不是指同一Job的两个或两个以上的task可以同时运行于同一JVM上，而是排队按顺序执行。

#### 算子的合理选择

- map和mapPartition的区别,rdd.map和rdd.mapPartition:

  map是对RDD中的每个元素作用上一个函数，1rdd=100partition=1w个元素

  mapPartition是将函数作用到partition之上的。

  如果遇到需要写数据到数据库，一定要选择该模式

- foreach和foreachPartition的区别：

  foreach：方法被传入了迭代器的foreach（每个元素便利执行一次函数），

  foreachPartition：方法被传入了迭代器的foreach（每个分区执行一次函数）

  总结：

  ​	假如我们的Function中有数据库，网络TCP等IO链接，文件流等等的创建关闭操作，采用foreachPatition方法，针对每个分区集合进行计算，更能提高我们的性能。

- groupbykey和reducebykey的区别：

  groupbykey不在map端集合（combine），只生成key-value，消耗IO员

  reducebykey回在map做一个本地的聚合，然后将集合的数据进行shuffle操作，减少IO

- transaction和action类算子的区别

  1，transformation是得到一个新的RDD，方式很多，比如从数据源生成一个新的RDD，从RDD生成一个新的RDD

  2，action是得到一个值，或者一个结果（直接将RDDcache到内存中）

  所有的transformation都是采用的懒策略，就是如果只是将transformation提交是不会执行计算的，计算只有在action被提交的时候才被触发。

  transformation操作：
    map(func):对调用map的RDD数据集中的每个element都使用func，然后返回一个新的RDD,这个返回的数据集是分布式的数据集

    filter(func): 对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD

  flatMap(func):和map差不多，但是flatMap生成的是多个结果

  mapPartitions(func):和map很像，但是map是每个element，而mapPartitions是每个partition

  mapPartitionsWithSplit(func):和mapPartitions很像，但是func作用的是其中一个split上，所以func中应该有index

  sample(withReplacement,faction,seed):抽样

  union(otherDataset)：返回一个新的dataset，包含源dataset和给定dataset的元素的集合

  distinct([numTasks]):返回一个新的dataset，这个dataset含有的是源dataset中的distinct的element

  groupByKey(numTasks):返回(K,Seq[V])，也就是hadoop中reduce函数接受的key-valuelist

  reduceByKey(func,[numTasks]):就是用一个给定的reducefunc再作用在groupByKey产生的(K,Seq[V]),比如求和，求平均数

  sortByKey([ascending],[numTasks]):按照key来进行排序，是升序还是降序，ascending是boolean类型

  join(otherDataset,[numTasks]):当有两个KV的dataset(K,V)和(K,W)，返回的是(K,(V,W))的dataset,numTasks为并发的任务数

  cogroup(otherDataset,[numTasks]):当有两个KV的dataset(K,V)和(K,W)，返回的是(K,Seq[V],Seq[W])的dataset,numTasks为并发的任务数

  cartesian(otherDataset)：笛卡尔积就是m*n，大家懂的

   

  action操作：
  reduce(func)：说白了就是聚集，但是传入的函数是两个参数输入返回一个值，这个函数必须是满足交换律和结合律的

  collect()：一般在filter或者足够小的结果的时候，再用collect封装返回一个数组

  count():返回的是dataset中的element的个数

  first():返回的是dataset中的第一个元素

  take(n):返回前n个elements，这个士driverprogram返回的

  takeSample(withReplacement，num，seed)：抽样返回一个dataset中的num个元素，随机种子seed

  saveAsTextFile（path）：把dataset写到一个textfile中，或者hdfs，或者hdfs支持的文件系统中，spark把每条记录都转换为一行记录，然后写到file中

  saveAsSequenceFile(path):只能用在key-value对上，然后生成SequenceFile写到本地或者hadoop文件系统

  countByKey()：返回的是key对应的个数的一个map，作用于一个RDD

  foreach(func):对dataset中的每个元素都使用func
  
- reducebykey和countbykey的区别

  reduceByKey: Transormation 类算子,  根据用户传入的聚合逻辑对数组内的数据进行聚合, 懒策略, 延迟计算

  countByKey: Action 类算子, 不需要用户传入聚合逻辑，直接对数组内的数据进行统计记录数, 触发计算

- collect：返回一个数组包含了RDD的所有元素，数组是放在本地内存（driver memory中的，如果数组较小的话还可以，但是如果很大内存就会不足（Out of memory），一定要慎用。如果一定要用的话可以save到本地磁盘或者数据库再看。

- coalesce和repartition的区别：他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现

  coalesce重新分区只减不增，不走shuffle，repartition一般用于重新分更多的区才使用的，可增可减

- cashe和persist的区别：

  直接调用cache()或者没有入参的persist()，效果是一样的，都是使用默认的storage level （merory only）

  果需要自定义缓存级别，可以通过persist传入StorageLevel类里面的设置好的对象使用。

  storageLavel各个字段的意思：

  useDisk：使用硬盘（外存）
  useMemory：使用内存
  useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。
  deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象
  replication：备份数（在多个节点上备份）
  <img src="D:\技术学习\learning\笔记\学习\我的笔记图片\1590057891(1).jpg" alt="1590057891(1)" style="zoom:200%;" />

#### 序列化

它提供了两个序列化库：

- Java序列化：默认情况下，Spark使用Java的ObjectOutputStream框架对对象进行序列化，并且可以与您创建的实现java.io.Serializable的任何类一起使用。 您还可以通过扩展java.io.Externalizable来更紧密地控制序列化的性能。 Java序列化很灵活，但是通常很慢，并且导致许多类的序列化格式很大。 

- Kryo序列化：Spark还可以使用Kryo库（版本4）更快地序列化对象。 与Java序列化（通常多达10倍）相比，Kryo显着更快，更紧凑，但是它不支持所有Serializable类型，并且需要您预先注册要在程序中使用的类才能获得最佳性能，如果您不注册自定义类，Kryo仍然可以使用，但必须将完整的类名与每个对象一起存储，这很浪费。

RDD.persist()使用memory_only_ser比memory_only更加节省空间



内存调优：![1590067831(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590067831(1).jpg)

​		Java对象的访问速度很快，但是与其字段中的“原始”数据相比，它们很容易消耗2-5倍的空间。这是由于以下几个原因：

- 每个不同的Java对象都有一个“对象头”，它大约16个字节，并包含诸如指向其类的指针之类的信息。对于其中数据很少的对象（例如一个`Int`字段），该对象可能大于数据。
- Java `String`相对于原始字符串数据有大约40个字节的开销（因为它们将其存储在`Char`s 数组中并保留诸如长度之类的额外数据），并且由于UTF-16的内部用法，因此将每个字符存储为*两个*字节`String`编码。因此，一个10个字符的字符串可以轻松消耗60个字节。
- 常见的收集类（例如`HashMap`和`LinkedList`）使用链接的数据结构，其中每个条目（例如`Map.Entry`）都有一个“包装”对象。该对象不仅具有标题，而且还具有指向列表中下一个对象的指针（通常每个指针8个字节）。
- 基本类型的集合通常将它们存储为“盒装”对象，例如`java.lang.Integer`。

#### 确定内存消耗

- 确定数据集所需的内存消耗量的最佳方法是创建一个RDD，将其放入缓存中，然后查看Web UI中的“ Storage”页面。该页面将告诉您RDD占用了多少内存。

####  调整数据结构

- 减少内存消耗的第一种方法是避免使用Java功能，这些功能会增加开销，例如基于指针的数据结构和包装对象。做这件事有很多种方法：

1. 将数据结构设计为更喜欢对象数组和原始类型，而不是标准Java或Scala集合类（例如`HashMap`）。该[fastutil](http://fastutil.di.unimi.it/) 库提供方便的集合类基本类型是与Java标准库兼容。
2. 尽可能避免使用带有许多小对象和指针的嵌套结构。
3. 考虑使用数字ID或枚举对象代替键的字符串。
4. 如果您的RAM少于32 GB，则设置JVM标志`-XX:+UseCompressedOops`以使指针为四个字节而不是八个字节。您可以在中添加这些选项 [`spark-env.sh`](http://spark.apache.org/docs/latest/configuration.html#environment-variables)。

#### 序列化RDD存储

- 当您的对象仍然太大而无法进行优化存储时，减少内存使用的一种更简单的方法是使用[RDD持久性API中](http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)的序列化（例如）以*序列化*形式存储它们。然后，Spark将每个RDD分区存储为一个大字节数组。由于必须动态地反序列化每个对象，因此以串行形式存储数据的唯一缺点是访问时间较慢。如果您想以序列化形式缓存数据，我们强烈建议[使用Kryo](http://spark.apache.org/docs/latest/tuning.html#data-serialization)，因为它导致的大小比Java序列化（当然也比原始Java对象）小。`MEMORY_ONLY_SER`
#### sink数据到mysql

- 错误示例

  ![1590069438(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590069438(1).jpg)

- 正确示例

  ![1590069519(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590069519(1).jpg)

#### 宽依赖和窄依赖

- 窄依赖： 
  指父RDD的每个分区只被子RDD的一个分区所使用，子RDD分区通常对应常数个父RDD分区

- 宽依赖：
  是指父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区（O(n),与数据规模有关） 

- 相比于依赖，窄依赖对优化很有利，主要基于以下两点：

   1，依赖往往对应着Shuffle操作，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输；而窄依赖的每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换。
   2，当RDD分区丢失时（某个节点故障），Spark会对数据进行重算
   3，对于窄依赖，由于父RDD的一个分区只对应一个子RDD分区，这样只需要重算和子RDD分区对应的父RDD分区即可，所以这个重算对数据的利用率是100%的
   4，对于依赖，重算的父RDD分区对应多个子RDD分区的，这样实际上父RDD中只有一部分的数据是被用于恢复这个丢失的子RDD分区的，另一部分对应子RDD的其他未丢失分区，这就造成了多余的计算；更一般的，宽依赖中子RDD分区通常来自多个父RDD分区，极端情况下，所有的父RDD分区都要进行重新计算。
#### 数据倾斜

- 产生的地方

  **groupByKey**、**countByKey**、**reduceByKey**、**join**都会

  都是相同的id分到同一个reduce上面去，然后再reduce里面完成count操作

  | **关键词**                                  | **情形**                           |                                     **后果** |
  | ------------------------------------------- | ---------------------------------- | -------------------------------------------: |
  | **Join**                                    | 其中一个表较小，但是key集中        | 分发到某一个或几个Reduce上的数据远高于平均值 |
  | 大表与大表，但是分桶的判断字段0值或空值过多 | 这些空值都由一个reduce处理，灰常慢 |                                              |
  | **group by**                                | group by 维度过小，某值的数量过多  |                     处理某值的reduce灰常耗时 |
  | **Count Distinct**                          | 某特殊值过多                       |                     处理此特殊值的reduce耗时 |

- 定位数据倾斜

  **1、**你在自己的程序里面**找找**，哪些地方用了会产生**shuffle**的算子，**groupByKey**、**countByKey**、**reduceByKey**、**join**

  **2、看log**

  log一般会报是在你的哪一行代码，导致了OOM异常；或者呢，看log，看看是执行到了第几个stage！！！哪一个stage，task特别慢，就能够自己用肉眼去对你的spark代码进行stage的划分，就能够通过stage定位到你的代码，哪里发生了数据倾斜。去找找，代码那个地方，是哪个shuffle操作。

- 解决方案：

  1，能先进行 group 操作的时候先进行 group 操作，把 key 先进行一次 reduce,之后再进行 count 或者 distinct count 操作。

  2，join 操作中，使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住。这种方式下，根本不会发生shuffle操作，肯定也不会发生数据倾斜；**从根本上杜绝了join操作可能导致的数据倾斜的问题**；

  3，**参数调节：**

  **hive.map.aggr=true**

  Map 端部分聚合，相当于Combiner

  **hive.groupby.skewindata****=true**

  有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。

  4，**SQL语句调节：**

  **如何****Join：**

  关于驱动表的选取，选用join key分布最均匀的表作为驱动表

  做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。

  **大小表****Join****：**

  使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.

  **大表****Join****大表：**

  把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。

  **count distinct****大量相同特殊值**

  count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。

  **group by****维度过小：**

  采用sum() group by的方式来替换count(distinct)完成计算。

  **特殊情况特殊处理：**

  在业务逻辑优化效果的不大情况下，有些时候是可以将倾斜的数据单独拿出来处理。最后union回去。
  
  5，**提高shuffle操作的reduce并行度**
  
  将**增加reduce task的数量**，就可以让每个reduce task分配到更少的数据量，这样的话，也许就可以缓解，或者甚至是基本解决掉数据倾斜的问题。主要给我们所有的shuffle算子，比如groupByKey、countByKey、reduceByKey。**在调用的时候，传入进去一个参数**。一个数字。那个数字，就**代表了那个shuffle操作的reduce端的并行度**。
  
  6，**随机key实现双重聚合**
  
  ![1590675598(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590675598(1).jpg)


## spark常见的面试题

![1590070385(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590070385(1).jpg)

![1590070432(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590070432(1).jpg)

![1590112370](D:\技术学习\learning\笔记\学习\我的笔记图片\1590112370.jpg)



#### Spark on yarn两种方式的区别以及工作流程

![1590472597(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590472597(1).jpg)

###### 工作流程

- YARN-Client：

  在Yarn-client中，Driver运行在Client上，通过ApplicationMaster向RM获取资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。结束掉终端，相当于kill掉这个spark应用。

   

  因为Driver在客户端，所以可以通过webUI访问Driver的状态，默认是http://hadoop1:4040访问，而YARN通过http:// hadoop1:8088访问

   

  YARN-client的工作流程步骤为：

## ![1590472488(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590472488(1).jpg)





![1590472530(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590472530(1).jpg)

1，Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend
2，ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派
3，Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）
4，一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task
5，client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务
6，应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己

因为是与Client端通信，所以Client不能关闭。

 	客户端的Driver将应用提交给Yarn后，Yarn会先后启动ApplicationMaster和executor，另外ApplicationMaster和executor都 是装载在container里运行，container默认的内存是1G，ApplicationMaster分配的内存是driver- memory，executor分配的内存是executor-memory。同时，因为Driver在客户端，所以程序的运行结果可以在客户端显 示，Driver以进程名为SparkSubmit的形式存在。

- Yarn-Cluster

  在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序：

  1. 第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动；

  2. 第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成

   应用的运行结果不能在客户端显示（可以在history server中查看），所以最好将结果保存在HDFS而非stdout输出，客户端的终端显示的是作为YARN的job的简单运行状况，下图是yarn-cluster模式
  
  ![779ea0a7b1a6ca4f07fb51067b918c4](D:\技术学习\learning\笔记\学习\我的笔记图片\779ea0a7b1a6ca4f07fb51067b918c4.png)

![1590474308(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590474308(1).jpg)

执行过程： 

1，Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等
2，ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化
3，ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束
4，一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，而Executor对象的创建及维护是由CoarseGrainedExecutorBackend负责的，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等
5，ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务
6，应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己

- YARN_Cluster和YARN_Client的区别

  1，理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别

  2，YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业

  3，YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开

（1）YarnCluster的Driver是在集群的某一台NM上，但是Yarn-Client就是在RM的机器上； 
（2）而Driver会和Executors进行通信，所以Yarn_cluster在提交App之后可以关闭Client，而Yarn-Client不可以； 
（3）Yarn-Cluster适合生产环境，Yarn-Client适合交互和调试。

 下表是Spark Standalone与Spark On Yarn模式下的比较

![1590475357(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590475357(1).jpg)



referren：https://www.cnblogs.com/ittangtang/p/7967386.html

#### Spark的内存管理

查看详细链接：https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html

#### spark的作业资源调度情况

reference   [https://andone1cc.github.io/2017/03/12/Spark/%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98/](https://andone1cc.github.io/2017/03/12/Spark/资源调优/)

#### shuffle的机制，依赖

窄依赖：英文全名narrow dependencies 。一个RDD对它的父RDD，只有简单的一对一的依赖关系，也就是说，RDD中的每个partition，仅仅依赖于父RDD中的一个partition，父RDD和子RDD的partition之间是一对一的关系。这种情况下，是简单的RDD之间的依赖关系，也被称之为窄依赖。

宽依赖：英文全名shuffle/wide dependence。本质就是shuffle，也就是说每一个父RDD中的partition中的数据，都可能会传输一部分到下一个RDD的每一个partition，也就是说，每一个父RDD和子RDD的partition之间，具有交互错杂的关系，那么这种情况就叫做两个RDD之间是宽依赖，同时他们之间发生的操作是shuffle。

![1590633477(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590633477(1).jpg)

总结如下：

窄依赖（ narrow dependencies ）

子 RDD 的每个分区依赖于常数个父分区（即与数据规模无关）

输入输出一对一的算子，且结果 RDD 的分区结构不变，主要是 map 、 flatMap

输入输出一对一，但结果 RDD 的分区结构发生了变化，如 union 、 coalesce(要求shuffle 参数为false)

从输入中选择部分元素的算子，如 filter 、 subtract 、 sample

宽依赖（shuffle/wide dependence ）

子 RDD 的每个分区依赖于所有父 RDD 分区

对单个RDD基于key进行重组和reduce，如groupByKey、distinct、reduceByKey

只要是宽依赖，就一定存在shuffle过程 ，接下来对Spark中的shuffle机制进行阐述。

shuffle指的就是数据的重新分配的一个过程，也就是说如果RDD的分区数据有交叉变化，那么这个变化过程中就存在着shuffle过程。只要是宽依赖，就一定存在shuffle过程， 两个stage之间一定存在shuffle过程。

Spark中的shuffle操作由shuffle manager负责，默认使用sort机制

#### RDD，Dataframe和DataSet的区别

- RDD：

  一个RDD就是你的数据的一个**不可变的分布式元素集合**，在集群中跨节点分布，可以通过若干提供了转换和处理的底层API进行并行处理。

  **使用RDD的场景**:

  - 你希望可以对你的数据集进行最基本的转换、处理和控制；
  - 你的数据是非结构化的，比如流媒体或者字符流；
  - 你想通过函数式编程而不是特定领域内的表达来处理你的数据；
  - 你不希望像进行列式处理一样定义一个模式，通过名字或字段来处理或访问数据属性；
  - 你并不在意通过DataFrame和Dataset进行结构化和半结构化数据处理所能获得的一些优化和性能上的好处；

  **优点**：

  1. 强大，内置很多函数操作，group，map，filter等，方便处理结构化或非结构化数据
  2. 面向对象编程，直接存储的java对象，类型转化也安全

  **缺点**：

  1. 由于它基本和hadoop一样万能的，因此没有针对特殊场景的优化，比如对于结构化数据处理相对于sql来比非常麻烦
  2. 默认采用的是java序列号方式，序列化结果比较大，而且数据存储在java堆内存中，导致gc比较频繁

  

- DataFrame：

  DataFrame是一种**以RDD为基础的分布式数据集，类似于传统数据库中的二维表格**。DataFrame引入了**schema**。

![1590647925(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590647925(1).jpg)

- **RDD和DataFrame比较**：

  1. **相同之处**：都是**不可变分布式弹性数据集**。
  2. **不同之处**：DataFrame的数据集都是按指定列存储，即**结构化数据**。类似于传统数据库中的表。

  上图直观地体现了DataFrame和RDD的区别。

  - 左侧的RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。
  - RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。
  - DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。

  **优点：**

  1. 结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表
  2. 有针对性的优化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数。
  3. hive兼容，支持hql，udf等

​    **缺点：**

1. 编译时不能类型转化安全检查，运行时才能确定是否有问题
2. 对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象





- 3 DataSet

> A **Dataset** is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each Dataset also has an untyped view called a **DataFrame**, which is a Dataset of **Row**.

Dataset是一个强类型的特定领域的对象，这种对象可以函数式或者关系操作并行地转换。每个Dataset都有一个称为DataFrame的非类型化的视图，这个视图是行的数据集。这种DataFrame是Row类型的Dataset，即Dataset[Row]。

你可以**把DataFrame当作一些通用对象Dataset[Row]的集合的一个别名**，而一行就是一个通用的**无类型的JVM对象**。与之形成对比，Dataset就是一些**有明确类型定义**的JVM对象的集合，通过你在Scala中定义的Case Class或者Java中的Class来指定。

Dataset是“懒惰”的，只在执行行动操作时触发计算。本质上，数据集表示一个逻辑计划，该计划描述了产生数据所需的计算。当执行行动操作时，Spark的查询优化程序优化逻辑计划，并生成一个高效的并行和分布式物理计划。

**DataSet和RDD主要的区别**是：DataSet是特定域的对象集合；然而RDD是任何对象的集合。DataSet的API总是强类型的；而且可以利用这些模式进行优化，然而RDD却不行。

**优点：**

1. dataset整合了rdd和dataframe的优点，支持结构化和非结构化数据
2. 和rdd一样，支持自定义对象存储
3. 和dataframe一样，支持结构化数据的sql查询
4. 采用堆外内存存储，gc友好
5. 类型转化安全，代码友好
6. 官方建议使用dataset

#### RDD的五大特性

1.A list of partitions
RDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。
2.A function for computing each split
RDD的每个partition上面都会有function，也就是函数应用，其作用是实现RDD之间partition的转换。
3.A list of dependencies on other RDDs
RDD会记录它的依赖 ，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。
4.Optionally,a Partitioner for Key-value RDDs
  可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面
5.Optionally, a list of preferred locations to compute each split on
最优的位置去计算，也就是数据的本地性。

\>>创建RDD有两种方式

1) 通过HDFS支持的文件系统创建RDD,RDD里面没有真正要计算的数据，只记录了一些元数据

2) 通过Scala集合或者数组并行化的方式创建RDD  

#### Spark的作业执行流程

![1590846650](D:\技术学习\learning\笔记\学习\我的笔记图片\1590846650.jpg)

1、RDD Objects

RDD（ResilientDistributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。

>>其中spark Rdd算子可分为两类：

1）transformation （转换） 延迟加载，只会记录元数据信息
2） action (动作) 当计算任务真正出发action时才会开始计算 其中当RDD进行一系列transformation操作后最终遇到Action方法时，DAG图即确定了边界，DAG图形成。


DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。

随后会将DAG提交给DAGScheduler.


2

2、DAGScheduler会将DAG切分成多个stage,切分依据(宽依赖—shuffledRDD—即数据需要网络传递) 

    >>RDD和它依赖的父RDD的关系有两种不同的类型，即窄依赖和宽依赖
        >>窄依赖指的是每一个父RDD的partition最多被子RDD的一个Partition使用。(独生子女)
        >>宽依赖指的是多个子RDD的partition会依赖同一个父RDD的Partition。(超生) 
![1590846886(1)](D:\技术学习\learning\笔记\学习\我的笔记图片\1590846886(1).jpg)

3、将多个stage封装到TaskSet后提交给TaskScheduler 。
4、随后TaskScheduler把任务提交给worker执行  。

注：其中DAGScheduler 和TaskScheduler都在Driver端（开启spark-shell的那一端），main函数创建SparkContext时会使得driver和Master节点建立连接，Master会根据任务所需资源在集群中找符合条件的worker. 随后Master对worker进行RPC通信，通知worker启动Executor ，Executor会和Driver 建立连接，随后的工作worker和Master不再有关系。 随后Driver会向Executor提交Task。

#### Spark和MR的区别

- MR是基于进程，spark是基于线程

- Spark的多个task跑在同一个进程上，这个进程会伴随spark应用程序的整个生命周期，即使没有作业进行，进程也是存在的

- MR的每一个task都是一个进程，当task完成时，进程也会结束

- 所以，spark比MR快的原因也在这，MR启动就需要申请资源，用完就销毁，但是spark把进程拿到以后，这个进程会一直存在，即使没有job在跑，所以后边的job可以直接启动，不需要再重新申请资源

- ### 速度

spark把运算的中间数据存放在内存，迭代计算效率更高；MR的中间结果需要落地，需要保存到磁盘，这样必然会有磁盘IO操作，影响性能

- ### **容错性**

spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在节点内存中的只读性质的数据集，这些集合石弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重建；MR的话容错可能只能重新计算了，成本较高

- ###  **适用面**

spark更加通用，spark提供了transformation和action这两大类的多个功能的api，另外还有流式处理sparkstreaming模块，图计算GraphX等；MR只提供了map和reduce两种操作，流计算以及其他模块的支持比较缺乏

- ### **框架和生态**

 Spark框架和生态更为复杂，首先由RDD、血缘lineage、执行时的有向无环图DAG、stage划分等等，

很多时候spark作业都需要根据不同的业务场景的需要进行调优，以达到性能要求，MR框架及其生态相对较为简单，对性能的要求也相对较弱，但是运行较为稳定，适合长期后台运行

 

- ### **运行环境：**

**MR**运行在YARN上，

**spark**

1. local：本地运行
2. standalone：使用Spark自带的资源管理框架，运行spark的应用
3. yarn：将spark应用类似mr一样，提交到yarn上运行
4. mesos：类似yarn的一种资源管理框架

#### Saprk如何解决OOM

reference：https://blog.csdn.net/yhb315279058/article/details/51035631



## JAVA

#### 反射之class详解

**得到class的几种方式：**

- Class.forname("test.Student"):参数为class的reference路径，运行静态代码，初始化类

- getClassLoader().loadClass("test.Student")：不运行静态代码

- object.getClass():"guanshuai".getClass()

- 类.class:String.class

**扩展**

​	如何再List<String> list里面加入Integer，可以调用该类的class，然后使用clazz.getDeclaredMethod("add", Object.class);然后再用method.invode加入Integer

```java
List<String> list = new ArrayList<>();list.add("guanshuai");
Class<?> clazz = list.getClass();
Method add = clazz.getDeclaredMethod("add", Object.class);//拿到List类的add方法
add.invoke(list,1234);
for(Object o: list){    
    System.out.println(o);
}
```

**反射之Constructor**

- class.getConstructors(),class.getConstructor(Class<?>... parameterTypes),使用consturctor的newInstance（）

**反射之method**

- Method mathod = class.getDeclaredMethod("Name",ParamType.class);method.invoke(object,params);

**反射之field详解**

- Field field = clazz.getDeclaredField("sex"), field.set(object,param)。
- 实例里面的private属性可以通过反射来设置，filed.setAccessble(true)来访问。

**BeanUtils详解**（org.apache.common.beanutils.BeanUtils)

使用集合中的属性来填充javabean里面的属性

- ![1590218237345](C:\Users\好里好比\AppData\Roaming\Typora\typora-user-images\1590218237345.png)